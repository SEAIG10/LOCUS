"""
ë°ì´í„°ì…‹ ê²€ì¦ ë° ì‹œê°í™”
ìƒì„±ëœ ë°ì´í„°ì…‹ì˜ í’ˆì§ˆì„ í™•ì¸í•©ë‹ˆë‹¤.
"""

import numpy as np
import os
import sys

def load_dataset(path: str):
    """ë°ì´í„°ì…‹ ë¡œë“œ"""
    data = np.load(path, allow_pickle=True)
    return data

def validate_dataset(dataset_path: str):
    """ë°ì´í„°ì…‹ ê²€ì¦"""
    print("=" * 60)
    print("Dataset Validation Report")
    print("=" * 60 + "\n")

    # ë°ì´í„° ë¡œë“œ
    data = load_dataset(dataset_path)
    X = data['X']
    y = data['y']
    metadata = data['metadata'].item() if 'metadata' in data else {}

    print(f"ğŸ“ Dataset: {os.path.basename(dataset_path)}")
    print(f"ğŸ’¾ File size: {os.path.getsize(dataset_path) / 1024 / 1024:.2f} MB\n")

    # 1. ê¸°ë³¸ ì •ë³´
    print("1ï¸âƒ£  Basic Information")
    print("-" * 60)
    print(f"Features shape: {X.shape}")
    print(f"Labels shape: {y.shape}")
    print(f"Total timesteps: {len(X):,}")
    print(f"Feature dimension: {X.shape[1]}")
    print(f"Number of zones: {y.shape[1]}")
    if metadata:
        print(f"Generation days: {metadata.get('num_days', 'N/A')}")
        print(f"Timesteps per hour: {metadata.get('timesteps_per_hour', 'N/A')}")
    print()

    # 2. íŠ¹ì„± í†µê³„
    print("2ï¸âƒ£  Feature Statistics")
    print("-" * 60)
    print(f"Mean: {X.mean():.4f}")
    print(f"Std: {X.std():.4f}")
    print(f"Min: {X.min():.4f}")
    print(f"Max: {X.max():.4f}")
    print(f"Non-zero ratio: {(X != 0).sum() / X.size:.2%}")
    print()

    # 3. ë¼ë²¨ í†µê³„ (êµ¬ì—­ë³„)
    print("3ï¸âƒ£  Label Statistics (Pollution by Zone)")
    print("-" * 60)
    zones = metadata.get('zones', ['zone_0', 'zone_1', 'zone_2', 'zone_3'])
    for i, zone in enumerate(zones):
        zone_pollution = y[:, i]
        print(f"{zone:15s}:")
        print(f"  Mean:   {zone_pollution.mean():.3f}")
        print(f"  Std:    {zone_pollution.std():.3f}")
        print(f"  Min:    {zone_pollution.min():.3f}")
        print(f"  Max:    {zone_pollution.max():.3f}")
        print(f"  Median: {np.median(zone_pollution):.3f}")

        # ë¶„í¬ ë¶„ì„
        clean = (zone_pollution < 0.3).sum()
        moderate = ((zone_pollution >= 0.3) & (zone_pollution < 0.7)).sum()
        dirty = (zone_pollution >= 0.7).sum()
        print(f"  Clean (<0.3):    {clean:6d} ({clean/len(zone_pollution)*100:5.1f}%)")
        print(f"  Moderate (0.3-0.7): {moderate:6d} ({moderate/len(zone_pollution)*100:5.1f}%)")
        print(f"  Dirty (>=0.7):   {dirty:6d} ({dirty/len(zone_pollution)*100:5.1f}%)")
        print()

    # 4. ì‹œê°„ ì‹œê³„ì—´ ê²€ì¦ (ì²« 100ê°œ ìƒ˜í”Œ)
    print("4ï¸âƒ£  Time Series Check (first 100 timesteps)")
    print("-" * 60)
    sample_size = min(100, len(y))

    # ê° êµ¬ì—­ì˜ ì˜¤ì—¼ë„ ë³€í™” í™•ì¸
    for i, zone in enumerate(zones):
        pollution_change = np.diff(y[:sample_size, i])
        increase_ratio = (pollution_change > 0).sum() / len(pollution_change)
        decrease_ratio = (pollution_change < 0).sum() / len(pollution_change)

        print(f"{zone:15s}: increase={increase_ratio:.1%}, decrease={decrease_ratio:.1%}, "
              f"stable={(pollution_change == 0).sum() / len(pollution_change):.1%}")
    print()

    # 5. ë°ì´í„° í’ˆì§ˆ ì²´í¬
    print("5ï¸âƒ£  Data Quality Checks")
    print("-" * 60)

    # NaN/Inf ì²´í¬
    has_nan_X = np.isnan(X).any()
    has_inf_X = np.isinf(X).any()
    has_nan_y = np.isnan(y).any()
    has_inf_y = np.isinf(y).any()

    print(f"âŒ NaN in features: {'YES (ERROR!)' if has_nan_X else 'NO (OK)'}")
    print(f"âŒ Inf in features: {'YES (ERROR!)' if has_inf_X else 'NO (OK)'}")
    print(f"âŒ NaN in labels: {'YES (ERROR!)' if has_nan_y else 'NO (OK)'}")
    print(f"âŒ Inf in labels: {'YES (ERROR!)' if has_inf_y else 'NO (OK)'}")

    # ë²”ìœ„ ì²´í¬ (ì˜¤ì—¼ë„ëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•¨)
    out_of_range = ((y < 0) | (y > 1)).sum()
    print(f"âŒ Labels out of range [0,1]: {out_of_range} ({out_of_range/y.size*100:.2f}%)")
    print()

    # 6. í´ë˜ìŠ¤ ì‚¬ìš© í˜„í™© (YOLO + YAMNet)
    if metadata and 'yolo_classes' in metadata and 'yamnet_classes' in metadata:
        print("6ï¸âƒ£  Class Usage (YOLO + YAMNet)")
        print("-" * 60)

        # YOLO í´ë˜ìŠ¤ (visual features, 14dim)
        # Feature êµ¬ì¡°: [time(10) + spatial(4) + visual(14) + audio(17) + pose(51) + padding(3)]
        visual_start = 14
        visual_end = 14 + 14
        visual_features = X[:, visual_start:visual_end]

        yolo_classes = metadata['yolo_classes']
        print("YOLO Classes:")
        for i, cls in enumerate(yolo_classes):
            usage = (visual_features[:, i] > 0).sum()
            print(f"  {cls:20s}: {usage:6d} timesteps ({usage/len(X)*100:5.1f}%)")
        print()

        # YAMNet í´ë˜ìŠ¤ (audio features, 17dim)
        audio_start = 14 + 14
        audio_end = 14 + 14 + 17
        audio_features = X[:, audio_start:audio_end]

        yamnet_classes = metadata['yamnet_classes']
        print("YAMNet Classes:")
        for i, cls in enumerate(yamnet_classes):
            usage = (audio_features[:, i] > 0).sum()
            print(f"  {cls:20s}: {usage:6d} timesteps ({usage/len(X)*100:5.1f}%)")
        print()

    print("=" * 60)
    print("âœ… Validation Complete!")
    print("=" * 60)


if __name__ == "__main__":
    # ìƒì„±ëœ ë°ì´í„°ì…‹ ê²€ì¦
    dataset_paths = [
        '/Users/inter4259/Projects/SE_G10/data/generated/realistic_dataset_100days.npz',
        '/Users/inter4259/Projects/SE_G10/data/generated/realistic_dataset_500days_50k.npz'
    ]

    for path in dataset_paths:
        if os.path.exists(path):
            validate_dataset(path)
            print("\n" + "="*60 + "\n")
        else:
            print(f"âš ï¸  Dataset not found: {path}\n")
