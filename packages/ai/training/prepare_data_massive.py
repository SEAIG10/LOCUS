"""
ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ì˜µì…˜ 2 ìë™ ì‹¤í–‰)
2000ì¼ Ã— 3 ì‹œë“œ = ~220,000 íƒ€ì„ìŠ¤í…
"""

import os
import sys
import numpy as np

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.dataset.realistic_dataset_generator import RealisticRoutineGenerator
from training.config import PATHS


def compute_pollution_occurred_labels(y: np.ndarray) -> np.ndarray:
    """ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€ ë¼ë²¨ ê³„ì‚°"""
    pollution_occurred = np.zeros(len(y), dtype=np.float32)

    for i in range(1, len(y)):
        if np.any(y[i] > y[i-1]):
            pollution_occurred[i] = 1.0

    if len(y) > 0 and np.any(y[0] > 0.1):
        pollution_occurred[0] = 1.0

    return pollution_occurred


def generate_multi_seed_dataset(num_days: int, num_seeds: int, timesteps_per_hour: int = 4):
    """ì—¬ëŸ¬ ì‹œë“œë¡œ ë°ì´í„° ìƒì„±"""
    print("\n" + "=" * 80)
    print("ëŒ€ê·œëª¨ ë‹¤ì¤‘ ì‹œë“œ ë°ì´í„°ì…‹ ìƒì„±")
    print("=" * 80)
    print(f"ì„¤ì •:")
    print(f"  - ì‹œë“œë‹¹ ë‚ ì§œ ìˆ˜: {num_days}ì¼")
    print(f"  - ì‹œë“œ ê°œìˆ˜: {num_seeds}ê°œ")
    print(f"  - ì‹œê°„ë‹¹ íƒ€ì„ìŠ¤í…: {timesteps_per_hour}ê°œ (15ë¶„ ê°„ê²©)")
    print(f"  - ì˜ˆìƒ ì´ íƒ€ì„ìŠ¤í…: ~{num_days * num_seeds * 24 * timesteps_per_hour * 1.15:,.0f}ê°œ")
    print(f"    (ìë™ ì²­ì†Œ íŠ¸ë¦¬ê±° í¬í•¨)")
    print("=" * 80 + "\n")

    all_data = {
        'time': [],
        'spatial': [],
        'visual': [],
        'audio': [],
        'pose': [],
        'y': [],
        'pollution_occurred': []
    }

    for seed_idx in range(num_seeds):
        seed = 42 + seed_idx

        print(f"\n{'â”€' * 80}")
        print(f"ì‹œë“œ {seed_idx + 1}/{num_seeds} (seed={seed}) ìƒì„± ì¤‘...")
        print(f"{'â”€' * 80}")

        generator = RealisticRoutineGenerator(seed=seed)

        dataset = generator.generate_dataset(
            num_days=num_days,
            timesteps_per_hour=timesteps_per_hour,
            output_path=None
        )

        for key in ['time', 'spatial', 'visual', 'audio', 'pose', 'y']:
            all_data[key].append(dataset[key])

        pollution_labels = compute_pollution_occurred_labels(dataset['y'])
        all_data['pollution_occurred'].append(pollution_labels)

        print(f"âœ“ ì‹œë“œ {seed} ì™„ë£Œ: {len(dataset['y']):,} íƒ€ì„ìŠ¤í…")

    print(f"\n{'=' * 80}")
    print("ëª¨ë“  ì‹œë“œ ë°ì´í„° ë³‘í•© ì¤‘...")
    print(f"{'=' * 80}")

    merged_dataset = {}
    for key in ['time', 'spatial', 'visual', 'audio', 'pose', 'y', 'pollution_occurred']:
        merged_dataset[key] = np.concatenate(all_data[key], axis=0)
        print(f"  {key:20s}: {merged_dataset[key].shape}")

    merged_dataset['metadata'] = {
        'num_days_per_seed': num_days,
        'num_seeds': num_seeds,
        'timesteps_per_hour': timesteps_per_hour,
        'total_timesteps': len(merged_dataset['y']),
        'total_days_equivalent': num_days * num_seeds,
        'seeds_used': list(range(42, 42 + num_seeds)),
        'num_zones': 4,
        'zones': ['balcony', 'bedroom', 'kitchen', 'living_room'],
        'feature_dims': {
            'time': 10,
            'spatial': 4,
            'visual': 14,
            'audio': 17,
            'pose': 51
        },
        'new_labels': {
            'pollution_occurred': 'ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€ (1: ë°œìƒ, 0: ë¯¸ë°œìƒ)'
        }
    }

    # í†µê³„
    y = merged_dataset['y']
    pollution_occurred = merged_dataset['pollution_occurred']

    print(f"\n{'=' * 80}")
    print("ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
    print(f"{'=' * 80}")
    print(f"ì´ íƒ€ì„ìŠ¤í…: {merged_dataset['metadata']['total_timesteps']:,}")
    print(f"ë“±ê°€ ì¼ìˆ˜: {merged_dataset['metadata']['total_days_equivalent']:,}ì¼")

    print(f"\n{'â”€' * 80}")
    print("êµ¬ì—­ë³„ ì˜¤ì—¼ë„ í†µê³„")
    print(f"{'â”€' * 80}")
    zones = ['balcony', 'bedroom', 'kitchen', 'living_room']
    for i, zone in enumerate(zones):
        print(f"  {zone:15s}: mean={y[:, i].mean():.3f}, std={y[:, i].std():.3f}, "
              f"min={y[:, i].min():.3f}, max={y[:, i].max():.3f}")

    print(f"\n{'â”€' * 80}")
    print("ì˜¤ì—¼ ë°œìƒ ì—¬ë¶€ í†µê³„")
    print(f"{'â”€' * 80}")
    num_polluted = int(pollution_occurred.sum())
    num_clean = len(pollution_occurred) - num_polluted
    print(f"  ì˜¤ì—¼ ë°œìƒ:     {num_polluted:,} íƒ€ì„ìŠ¤í… ({num_polluted/len(pollution_occurred)*100:.1f}%)")
    print(f"  ì˜¤ì—¼ ë¯¸ë°œìƒ:   {num_clean:,} íƒ€ì„ìŠ¤í… ({num_clean/len(pollution_occurred)*100:.1f}%)")
    print(f"  í•©ê³„:          {len(pollution_occurred):,} íƒ€ì„ìŠ¤í…")

    return merged_dataset


if __name__ == "__main__":
    print("\n" + "â•”" + "â•" * 78 + "â•—")
    print("â•‘" + " " * 15 + "ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìë™ ìƒì„±" + " " * 30 + "â•‘")
    print("â•š" + "â•" * 78 + "â•")
    print("\nì˜µì…˜ 2: 2000ì¼ Ã— 3 ì‹œë“œ (ì•½ 220,000 íƒ€ì„ìŠ¤í…)\n")

    # ì˜µì…˜ 2 ìë™ ì„¤ì •
    num_days = 2000
    num_seeds = 3
    output_filename = 'massive_dataset_2000days_3seeds.npz'

    output_path = os.path.join(PATHS['data_dir'], output_filename)

    if os.path.exists(output_path):
        print(f"âš ï¸  ê¸°ì¡´ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤: {output_path}")
        print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB")
        print("\nê¸°ì¡´ íŒŒì¼ì„ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...\n")

    # ë°ì´í„° ìƒì„±
    dataset = generate_multi_seed_dataset(
        num_days=num_days,
        num_seeds=num_seeds,
        timesteps_per_hour=4
    )

    # ì €ì¥
    print(f"\n{'=' * 80}")
    print("ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
    print(f"{'=' * 80}")
    print(f"ê²½ë¡œ: {output_path}")

    np.savez_compressed(output_path, **dataset)

    file_size_mb = os.path.getsize(output_path) / 1024 / 1024

    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“ {output_path}")
    print(f"  ğŸ“Š ì´ íƒ€ì„ìŠ¤í…: {len(dataset['y']):,}")
    print(f"  ğŸ’¾ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")
    print(f"  ğŸ—œï¸  ì••ì¶•ë¥ : ~{len(dataset['y']) * 96 * 4 / 1024 / 1024 / file_size_mb:.1f}x")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. python training/train_encoder.py")
    print(f"  2. python training/train_gru.py")