"""
GRU ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (FedPer Structure)

í˜„ì‹¤ì ì¸ ì¼ìƒ ë£¨í‹´ ë°ì´í„°ì…‹ìœ¼ë¡œ GRU ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
í•™ìŠµ ëª©í‘œ: 160ì°¨ì› context ì‹œí€€ìŠ¤ (30 timesteps) â†’ 4ê°œ êµ¬ì—­ ì˜¤ì—¼ë„ ì˜ˆì¸¡ (regression)

FedPer êµ¬ì¡°:
- Base Layer (Shared): AttentionEncoder (99dim â†’ 160dim)  # ì‚¬ì „ í•™ìŠµë¨, frozen
- Head Layer (Personalized): GRU (160dim seq â†’ 4-zone prediction)  # ì´ë²ˆì— í•™ìŠµ
"""

import os
import sys
import numpy as np
import tensorflow as tf
from tensorflow.keras import callbacks
import matplotlib.pyplot as plt

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.model.gru_model import FedPerGRUModel
from src.context_fusion.attention_context_encoder import AttentionContextEncoder
from training.config import GRU_CONFIG, GRU_TRAINING, PATHS


def create_sequences_from_features(features_dict, labels, encoder, sequence_length=30):
    """
    ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ featuresë¥¼ AttentionEncoderë¥¼ ê±°ì³ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        features_dict: {
            'time': (N, 10),
            'spatial': (N, 4),
            'visual': (N, 14),
            'audio': (N, 17),
            'pose': (N, 51)
        }
        labels: (N, 4)
        encoder: ì‚¬ì „ í•™ìŠµëœ AttentionEncoder
        sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´

    Returns:
        X_seq: (N-sequence_length, sequence_length, 160)
        y_seq: (N-sequence_length, 4)
    """
    print("  [1/3] AttentionEncoderë¥¼ í†µê³¼ì‹œì¼œ context vectors ìƒì„± ì¤‘...")

    # AttentionEncoderë¥¼ ê±°ì³ 160dim context vectors ìƒì„±
    context_vectors = encoder.predict(features_dict, batch_size=512, verbose=0)
    print(f"       Context vectors shape: {context_vectors.shape}")

    print("  [2/3] Context vectorsë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ ì¤‘...")
    X_sequences = []
    y_sequences = []

    for i in range(len(context_vectors) - sequence_length):
        X_sequences.append(context_vectors[i:i+sequence_length])
        y_sequences.append(labels[i+sequence_length])  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ë¼ë²¨

    X_seq = np.array(X_sequences)
    y_seq = np.array(y_sequences)

    print(f"  [3/3] ìƒì„±ëœ ì‹œí€€ìŠ¤ shape: {X_seq.shape}")

    return X_seq, y_seq


class GRUTrainer:
    """GRU ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤ (FedPer Head Layer)"""

    def __init__(self, encoder_path=None, num_zones=4, context_dim=160):
        """
        Args:
            encoder_path: ì‚¬ì „ í•™ìŠµëœ AttentionEncoder ê²½ë¡œ
            num_zones: êµ¬ì—­ ê°œìˆ˜ (balcony, bedroom, kitchen, living_room)
            context_dim: Context ë²¡í„° ì°¨ì› (AttentionEncoder ì¶œë ¥)
        """
        self.num_zones = num_zones
        self.context_dim = context_dim
        self.gru_model = None

        # AttentionEncoder ë¡œë“œ (Frozen Base Layer)
        if encoder_path is None:
            encoder_path = PATHS['encoder_model']

        print("\n" + "=" * 70)
        print("ì‚¬ì „ í•™ìŠµëœ AttentionEncoder ë¡œë“œ ì¤‘...")
        print("=" * 70)

        if not os.path.exists(encoder_path):
            raise FileNotFoundError(
                f"AttentionEncoderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {encoder_path}\n"
                f"ë¨¼ì € 'python training/train_encoder.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
            )

        self.encoder = tf.keras.models.load_model(
            encoder_path,
            custom_objects={'AttentionContextEncoder': AttentionContextEncoder}
        )
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {encoder_path}\n")

        # Encoderë¥¼ frozen (í•™ìŠµ ì•ˆ í•¨)
        self.encoder.trainable = False
        print("ğŸ”’ AttentionEncoder frozen (Base Layer - í•™ìŠµ ì•ˆ í•¨)\n")

    def build_gru_model(self, sequence_length=30):
        """GRU ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤ (Head Layer)."""
        print("\n" + "=" * 70)
        print("GRU ëª¨ë¸ êµ¬ì¶• (FedPer Head Layer)")
        print("=" * 70 + "\n")

        self.gru_model = FedPerGRUModel(
            num_zones=self.num_zones,
            context_dim=self.context_dim  # 160ì°¨ì› context ì…ë ¥
        )

        # ë”ë¯¸ ë°ì´í„°ë¡œ ëª¨ë¸ ë¹Œë“œ
        dummy_input = tf.random.normal((1, sequence_length, self.context_dim))
        _ = self.gru_model.model(dummy_input)

        print("[GRU ëª¨ë¸ ì•„í‚¤í…ì²˜]")
        self.gru_model.summary()
        print("\n" + "=" * 70 + "\n")

    def compile_model(self):
        """GRU ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤ (Regression ìš©)."""
        self.gru_model.compile_model(
            learning_rate=GRU_TRAINING['learning_rate'],
            loss='mse',  # Regression: Mean Squared Error
            metrics=['mae', 'mse']  # Mean Absolute Error, MSE
        )
        print("GRU ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ (Regression)\n")

    def train(self, X_train, y_train, X_val, y_val):
        """
        GRU ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

        Args:
            X_train: (N_train, 30, 160) - Context vector sequences
            y_train: (N_train, 4)
            X_val: (N_val, 30, 160)
            y_val: (N_val, 4)

        Returns:
            history: í•™ìŠµ ê¸°ë¡
        """
        print("\n" + "=" * 70)
        print("GRU ëª¨ë¸ í•™ìŠµ ì‹œì‘ (FedPer Head Layer)")
        print("=" * 70 + "\n")

        print(f"í›ˆë ¨ ìƒ˜í”Œ: {X_train.shape[0]:,}ê°œ")
        print(f"ê²€ì¦ ìƒ˜í”Œ: {X_val.shape[0]:,}ê°œ")
        print(f"ì…ë ¥ shape: {X_train.shape}")
        print(f"ì¶œë ¥ shape: {y_train.shape}\n")

        # ì½œë°± ì„¤ì •
        callback_list = [
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=GRU_TRAINING['early_stopping_patience'],
                restore_best_weights=True,
                verbose=1
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=GRU_TRAINING['reduce_lr_patience'],
                min_lr=GRU_TRAINING['min_lr'],
                verbose=1
            ),
            callbacks.ModelCheckpoint(
                filepath=PATHS['gru_model'],
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            )
        ]

        # í•™ìŠµ
        history = self.gru_model.train(
            X_train, y_train,
            X_val, y_val,
            epochs=GRU_TRAINING['epochs'],
            batch_size=GRU_TRAINING['batch_size'],
            callbacks=callback_list,
            verbose=1
        )

        print("\n" + "=" * 70)
        print("í•™ìŠµ ì™„ë£Œ!")
        print("=" * 70 + "\n")

        return history

    def evaluate(self, X_val, y_val, zone_names):
        """
        GRU ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            X_val: ê²€ì¦ ì…ë ¥
            y_val: ê²€ì¦ ë¼ë²¨
            zone_names: êµ¬ì—­ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        print("\n" + "=" * 70)
        print("GRU ëª¨ë¸ í‰ê°€")
        print("=" * 70 + "\n")

        # ì˜ˆì¸¡
        y_pred = self.gru_model.predict(X_val)

        # êµ¬ì—­ë³„ í‰ê°€
        print("êµ¬ì—­ë³„ ì„±ëŠ¥ (Regression):")
        print("-" * 70)
        for i, zone in enumerate(zone_names):
            mae = np.mean(np.abs(y_val[:, i] - y_pred[:, i]))
            mse = np.mean((y_val[:, i] - y_pred[:, i]) ** 2)
            rmse = np.sqrt(mse)

            print(f"{zone:15s}: MAE={mae:.4f}, MSE={mse:.4f}, RMSE={rmse:.4f}")

        # ì „ì²´ í‰ê°€
        overall_mae = np.mean(np.abs(y_val - y_pred))
        overall_rmse = np.sqrt(np.mean((y_val - y_pred) ** 2))

        print("-" * 70)
        print(f"{'Overall':15s}: MAE={overall_mae:.4f}, RMSE={overall_rmse:.4f}")
        print("=" * 70 + "\n")

    def save_model(self, save_path=None):
        """
        í•™ìŠµëœ GRU ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            save_path: ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: config)
        """
        if save_path is None:
            save_path = PATHS['gru_model']

        self.gru_model.save(save_path)
        print(f"âœ… GRU ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")

        # ëª¨ë¸ í¬ê¸° ì •ë³´
        file_size_mb = os.path.getsize(save_path) / (1024 * 1024)
        print(f"  íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")


def plot_training_history(history, save_path=None):
    """
    í•™ìŠµ ê¸°ë¡ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

    Args:
        history: Keras í•™ìŠµ ê¸°ë¡
        save_path: ì €ì¥ ê²½ë¡œ
    """
    if save_path is None:
        save_path = PATHS['gru_history']

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # ì†ì‹¤
    axes[0].plot(history.history['loss'], label='Train Loss (MSE)')
    axes[0].plot(history.history['val_loss'], label='Val Loss (MSE)')
    axes[0].set_title('GRU Training Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('MSE')
    axes[0].legend()
    axes[0].grid(True)

    # MAE
    if 'mae' in history.history:
        axes[1].plot(history.history['mae'], label='Train MAE')
        axes[1].plot(history.history['val_mae'], label='Val MAE')
        axes[1].set_title('GRU Training MAE')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('MAE')
        axes[1].legend()
        axes[1].grid(True)

    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š í•™ìŠµ ê¸°ë¡ ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")


def save_training_metrics(history, save_path: str = None):
    """
    í•™ìŠµ ì§€í‘œë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        history: Keras í•™ìŠµ ê¸°ë¡
        save_path: ì €ì¥ ê²½ë¡œ
    """
    if save_path is None:
        save_path = os.path.join(PATHS['results_dir'], 'gru_metrics.txt')

    with open(save_path, 'w') as f:
        f.write("=" * 70 + "\n")
        f.write("GRU ëª¨ë¸ í•™ìŠµ ê²°ê³¼ (FedPer Head Layer)\n")
        f.write("=" * 70 + "\n\n")

        # ìµœì¢… ì—í¬í¬ ì •ë³´
        final_epoch = len(history.history['loss'])
        f.write(f"ì´ í•™ìŠµ ì—í¬í¬: {final_epoch}\n\n")

        # ìµœê³  ì„±ëŠ¥
        f.write("ìµœê³  ì„±ëŠ¥ (Validation):\n")
        f.write("-" * 70 + "\n")
        best_epoch = np.argmin(history.history['val_loss']) + 1
        best_val_loss = min(history.history['val_loss'])
        best_val_mae = history.history['val_mae'][best_epoch - 1] if 'val_mae' in history.history else None

        f.write(f"  Best Epoch: {best_epoch}\n")
        f.write(f"  Val Loss (MSE): {best_val_loss:.6f}\n")
        if best_val_mae:
            f.write(f"  Val MAE: {best_val_mae:.6f}\n")
        f.write("\n")

        # ìµœì¢… ì„±ëŠ¥
        f.write("ìµœì¢… ì„±ëŠ¥:\n")
        f.write("-" * 70 + "\n")
        f.write(f"  Train Loss: {history.history['loss'][-1]:.6f}\n")
        f.write(f"  Val Loss: {history.history['val_loss'][-1]:.6f}\n")
        if 'mae' in history.history:
            f.write(f"  Train MAE: {history.history['mae'][-1]:.6f}\n")
            f.write(f"  Val MAE: {history.history['val_mae'][-1]:.6f}\n")
        f.write("\n")

        # ì—í¬í¬ë³„ ìƒì„¸ ê¸°ë¡
        f.write("ì—í¬í¬ë³„ ìƒì„¸ ê¸°ë¡:\n")
        f.write("-" * 70 + "\n")
        f.write(f"{'Epoch':>6} {'Train Loss':>12} {'Val Loss':>12} {'Train MAE':>12} {'Val MAE':>12}\n")
        f.write("-" * 70 + "\n")

        for i in range(final_epoch):
            train_loss = history.history['loss'][i]
            val_loss = history.history['val_loss'][i]
            train_mae = history.history['mae'][i] if 'mae' in history.history else 0
            val_mae = history.history['val_mae'][i] if 'val_mae' in history.history else 0

            f.write(f"{i+1:6d} {train_loss:12.6f} {val_loss:12.6f} {train_mae:12.6f} {val_mae:12.6f}\n")

        f.write("\n" + "=" * 70 + "\n")

    print(f"ğŸ“ í•™ìŠµ ì§€í‘œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {save_path}")


def main():
    """ë©”ì¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    print("\n" + "=" * 70)
    print("GRU ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (FedPer Head Layer)")
    print("=" * 70)

    # ===== ë‹¨ê³„ 1: ë°ì´í„° ë¡œë“œ =====
    print("\n[ë‹¨ê³„ 1] ë°ì´í„° ë¡œë“œ ì¤‘...")

    # ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©
    massive_data_path = os.path.join(PATHS['data_dir'], 'massive_dataset_2000days_3seeds.npz')
    default_data_path = os.path.join(PATHS['data_dir'], 'realistic_training_dataset.npz')

    if os.path.exists(massive_data_path):
        data_path = massive_data_path
        print(f"âœ“ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì‚¬ìš©: {data_path}")
    elif os.path.exists(default_data_path):
        data_path = default_data_path
        print(f"âœ“ ê¸°ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©: {data_path}")
    else:
        data_path = default_data_path

    if not os.path.exists(data_path):
        print(f"\nâš ï¸  ì˜¤ë¥˜: í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        print("ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”:")
        print("  python training/prepare_data.py")
        return

    data = np.load(data_path, allow_pickle=True)

    # Features ë¡œë“œ
    features_all = {
        'time': data['time'],
        'spatial': data['spatial'],
        'visual': data['visual'],
        'audio': data['audio'],
        'pose': data['pose']
    }
    labels_all = data['y']
    metadata = data['metadata'].item()

    print(f"  âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"  Total timesteps: {len(labels_all):,}")
    print(f"  Zones: {metadata['zones']}")

    # ===== ë‹¨ê³„ 2: GRU Trainer ì´ˆê¸°í™” (AttentionEncoder ë¡œë“œ) =====
    print("\n[ë‹¨ê³„ 2] GRU Trainer ì´ˆê¸°í™” ì¤‘ (AttentionEncoder ë¡œë“œ)...")
    trainer = GRUTrainer(num_zones=4, context_dim=160)

    # ===== ë‹¨ê³„ 3: Train/Val Split =====
    print("\n[ë‹¨ê³„ 3] Train/Val Split...")

    train_split = 0.8
    n_train = int(len(labels_all) * train_split)

    features_train = {key: value[:n_train] for key, value in features_all.items()}
    features_val = {key: value[n_train:] for key, value in features_all.items()}
    labels_train = labels_all[:n_train]
    labels_val = labels_all[n_train:]

    print(f"  í›ˆë ¨ ë°ì´í„°: {len(labels_train):,}ê°œ")
    print(f"  ê²€ì¦ ë°ì´í„°: {len(labels_val):,}ê°œ")

    # ===== ë‹¨ê³„ 4: ì‹œí€€ìŠ¤ ìƒì„± (AttentionEncoder í†µê³¼) =====
    print("\n[ë‹¨ê³„ 4] ì‹œí€€ìŠ¤ ìƒì„± (AttentionEncoder í†µê³¼)...")

    sequence_length = 30

    print("\ní›ˆë ¨ ë°ì´í„°:")
    X_train, y_train = create_sequences_from_features(
        features_train, labels_train, trainer.encoder, sequence_length
    )

    print("\nê²€ì¦ ë°ì´í„°:")
    X_val, y_val = create_sequences_from_features(
        features_val, labels_val, trainer.encoder, sequence_length
    )

    print(f"\nìµœì¢… shape:")
    print(f"  X_train: {X_train.shape}")
    print(f"  y_train: {y_train.shape}")
    print(f"  X_val: {X_val.shape}")
    print(f"  y_val: {y_val.shape}")

    # ===== ë‹¨ê³„ 5: GRU ëª¨ë¸ êµ¬ì¶• =====
    print("\n[ë‹¨ê³„ 5] GRU ëª¨ë¸ êµ¬ì¶• ì¤‘...")
    trainer.build_gru_model(sequence_length=sequence_length)

    # ===== ë‹¨ê³„ 6: ëª¨ë¸ ì»´íŒŒì¼ =====
    print("\n[ë‹¨ê³„ 6] ëª¨ë¸ ì»´íŒŒì¼ ì¤‘...")
    trainer.compile_model()

    # ===== ë‹¨ê³„ 7: í•™ìŠµ =====
    print("\n[ë‹¨ê³„ 7] í•™ìŠµ ì‹œì‘...")
    history = trainer.train(X_train, y_train, X_val, y_val)

    # ===== ë‹¨ê³„ 8: í‰ê°€ =====
    print("\n[ë‹¨ê³„ 8] ëª¨ë¸ í‰ê°€...")
    trainer.evaluate(X_val, y_val, zone_names=metadata['zones'])

    # ===== ë‹¨ê³„ 9: ëª¨ë¸ ì €ì¥ =====
    print("\n[ë‹¨ê³„ 9] GRU ëª¨ë¸ ì €ì¥...")
    trainer.save_model()

    # ===== ë‹¨ê³„ 10: í•™ìŠµ ê¸°ë¡ ì‹œê°í™” ë° ì €ì¥ =====
    print("\n[ë‹¨ê³„ 10] í•™ìŠµ ê¸°ë¡ ì‹œê°í™” ë° ì €ì¥...")
    plot_training_history(history)
    save_training_metrics(history)

    print("\n" + "=" * 70)
    print("âœ… GRU ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 70)
    print("\nì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“ {PATHS['gru_model']} (í•™ìŠµëœ GRU ëª¨ë¸ - Head Layer)")
    print(f"  ğŸ“Š {PATHS['gru_history']} (í•™ìŠµ ê·¸ë˜í”„)")
    print(f"  ğŸ“ {os.path.join(PATHS['results_dir'], 'gru_metrics.txt')} (ì„±ëŠ¥ ì§€í‘œ)")
    print("\nFedPer êµ¬ì¡° ì™„ì„±:")
    print(f"  ğŸ”’ Base Layer (Shared): {PATHS['encoder_model']}")
    print(f"  ğŸ¯ Head Layer (Personalized): {PATHS['gru_model']}")
    print("\ní•™ìŠµ ì™„ë£Œ! ì´ì œ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()